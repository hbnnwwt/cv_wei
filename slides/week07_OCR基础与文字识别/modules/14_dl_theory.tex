%===========================================================
% modules/14_dl_theory.tex - 深度学习OCR原理
%===========================================================

\section{深度学习OCR原理}

\begin{frame}{深度学习OCR架构演进}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{第一代：CNN单字识别}
        \begin{itemize}
            \item LeNet-5 (1998)
            \item AlexNet (2012)
            \item VGGNet (2014)
            \item ResNet (2015)
        \end{itemize}

        \textbf{特点：}
        \begin{itemize}
            \item 固定输入尺寸
            \item 单字分类
            \item 需字符分割
        \end{itemize}

        \column{0.5\textwidth}
        \textbf{第二代：CNN+RNN+CTC}
        \begin{itemize}
            \item CRNN (2015)
            \item CTPN (2016)
            \item SegLink (2017)
        \end{itemize}

        \textbf{特点：}
        \begin{itemize}
            \item 序列建模
            \item 端到端训练
            \item 无需字符分割
        \end{itemize}

        \vspace{0.3cm}

        \textbf{第三代：Transformer}
        \begin{itemize}
            \item TrOCR (2021)
            \item SRN (2020)
            \item Vision-Language预训练
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}{CNN特征提取原理}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{卷积层（Convolution）：}
        \begin{itemize}
            \item 局部连接：感受野
            \item 权值共享：平移不变性
            \item 特征图：$H \times W \times C$
        \end{itemize}

        \vspace{0.2cm}

        \textbf{池化层（Pooling）：}
        \begin{itemize}
            \item 最大池化：保留显著特征
            \item 平均池化：保留背景信息
            \item 下采样：降低维度
        \end{itemize}

        \vspace{0.2cm}

        \textbf{激活函数：}
        \begin{itemize}
            \item ReLU：$f(x) = \max(0, x)$
            \item Sigmoid：$f(x) = \frac{1}{1+e^{-x}}$
            \item Tanh：$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
        \end{itemize}

        \column{0.5\textwidth}
        \textbf{OCR中的CNN架构：}

        \vspace{0.2cm}

        \textbf{VGG-style（CRNN）：}
        \begin{lstlisting}[basicstyle=\ttfamily\tiny]
Conv(64) -> MaxPool
Conv(128) -> MaxPool
Conv(256) -> Conv(256) -> MaxPool
Conv(512) -> Conv(512) -> MaxPool
Conv(512) -> Conv(512) -> MaxPool
        \end{lstlisting}

        \vspace{0.2cm}

        \textbf{ResNet-style：}
        \begin{itemize}
            \item 残差连接：$y = F(x) + x$
            \item 解决梯度消失
            \item 支持更深网络
        \end{itemize}

        \vspace{0.2cm}

        \textbf{MobileNet-style：}
        \begin{itemize}
            \item 深度可分离卷积
            \item 轻量级设计
            \item 移动端部署
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}{RNN序列建模原理}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{为什么OCR需要RNN？}
        \begin{itemize}
            \item 文字是序列数据
            \item 上下文关联性强
            \item 变长输入输出
            \item 字符间依赖关系
        \end{itemize}

        \vspace{0.3cm}

        \textbf{标准RNN的局限：}
        \begin{itemize}
            \item 梯度消失/爆炸
            \item 长程依赖困难
            \item 信息传递衰减
        \end{itemize}

        \column{0.5\textwidth}
        \textbf{LSTM（长短期记忆网络）：}

        \vspace{0.2cm}

        LSTM通过门控机制解决RNN的局限：

        \vspace{0.2cm}

        \textbf{1. 遗忘门（Forget Gate）：}
        \begin{equation*}
            f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
        \end{equation*}
        决定丢弃什么信息

        \vspace{0.2cm}

        \textbf{2. 输入门（Input Gate）：}
        \begin{align*}
            i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
            \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
        \end{align*}
        决定存储什么新信息

        \vspace{0.2cm}

        \textbf{3. 细胞状态更新：}
        \begin{equation*}
            C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
        \end{equation*}

        \vspace{0.2cm}

        \textbf{4. 输出门（Output Gate）：}
        \begin{align*}
            o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
            h_t &= o_t \odot \tanh(C_t)
        \end{align*}
        决定输出什么信息
    \end{columns}
\end{frame}

\begin{frame}{Transformer for OCR}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{为什么Transformer适合OCR？}
        \begin{itemize}
            \item 全局依赖建模
            \item 并行计算高效
            \item 长程依赖捕获
            \item 可解释性强
        \end{itemize}

        \vspace{0.3cm}

        \textbf{OCR中的Transformer架构：}
        \begin{enumerate}
            \item \textbf{Encoder-Only}：
            \begin{itemize}
                \item ViT-style
                \item 图像分块编码
                \item 单阶段识别
            \end{itemize}
            \item \textbf{Encoder-Decoder}：
            \begin{itemize}
                \item TrOCR
                \item 图像编码+文本解码
                \item 自回归生成
            \end{itemize}
        \end{enumerate}

        \column{0.5\textwidth}
        \textbf{TrOCR（Transformer OCR）：}

        \vspace{0.2cm}

        \textbf{架构：}
        \begin{lstlisting}[basicstyle=\ttfamily\tiny]
# Image Encoder (BEiT/DeiT)
Image Patch Embedding
+ Position Embedding
+ Transformer Encoder Layers
-> Image Features

# Text Decoder (GPT-style)
Token Embedding
+ Position Embedding
+ Causal Mask
+ Transformer Decoder Layers
+ Linear + Softmax
-> Text Tokens
        \end{lstlisting}

        \vspace{0.2cm}

        \textbf{训练策略：}
        \begin{enumerate}
            \item \textbf{Pre-training}：
            \begin{itemize}
                \item 大规模合成数据
                \item 掩码图像建模
                \item 视觉-语言预训练
            \end{itemize}
            \item \textbf{Fine-tuning}：
            \begin{itemize}
                \item 真实场景数据
                \item 特定领域适应
                \item 小样本学习
            \end{itemize}
        \end{enumerate}

        \vspace{0.2cm}

        \textbf{Transformer OCR优势：}
        \begin{itemize}
            \item 端到端训练，无需组件
            \item 预训练知识迁移
            \item 多语言支持
            \item 手写识别效果好
        \end{itemize}
    \end{columns}
\end{frame}
